{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install rich scikit-learn tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix protobuf and TensorFlow compatibility issues\n",
    "!pip uninstall protobuf -y --quiet\n",
    "!pip install protobuf==3.20.3 --quiet\n",
    "!pip install tensorflow --upgrade --quiet\n",
    "\n",
    "print(\"Protobuf and TensorFlow updated. Please restart the kernel/runtime if you encounter import errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba780fcb",
   "metadata": {},
   "source": [
    "# Waste Classification Training on Kaggle\n",
    "\n",
    "This notebook trains a waste classification model using TensorFlow/Keras on Kaggle's free GPU.\n",
    "\n",
    "## Setup\n",
    "1. Add these datasets as input:\n",
    "   - garbage-classification (asdasdasasdas)\n",
    "   - new-trash-classfication-dataset (glhdamar)\n",
    "   - realwaste (joebeachcapital)\n",
    "   - garbage-classification (mostafaabla)\n",
    "   - garbage-classification (karansolanki01)\n",
    "   - garbage-classification-v2 (sumn2u)\n",
    "\n",
    "2. Set accelerator to GPU in notebook settings\n",
    "\n",
    "## Important: Fix TensorFlow Errors\n",
    "If you see protobuf or MessageFactory errors:\n",
    "1. Run Cell 2 (protobuf fix) first\n",
    "2. Restart runtime: Runtime â†’ Restart Runtime\n",
    "3. Run Cell 3 (install dependencies)\n",
    "4. Continue with other cells\n",
    "\n",
    "The errors are due to TensorFlow/protobuf version conflicts - the fixes resolve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from rich import print\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aba7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "class TrainConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.image_size = kwargs.get('image_size', 224)\n",
    "        self.batch_size = kwargs.get('batch_size', 64)\n",
    "        self.epochs = kwargs.get('epochs', 15)\n",
    "        self.validation_split = kwargs.get('validation_split', 0.2)\n",
    "        self.fine_tune_from = kwargs.get('fine_tune_from', 100)\n",
    "        self.mixed_precision = kwargs.get('mixed_precision', False)\n",
    "        self.model_dir = kwargs.get('model_dir', '/kaggle/working/models')\n",
    "        self.datasets = kwargs.get('datasets', [])\n",
    "        self.no_class_weights = kwargs.get('no_class_weights', False)\n",
    "        self.repr_samples = kwargs.get('repr_samples', 100)\n",
    "        self.brightness_factor = kwargs.get('brightness_factor', 0.1)\n",
    "        self.contrast_factor = kwargs.get('contrast_factor', 0.1)\n",
    "        self.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e47deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical classes mapping\n",
    "def get_canonical_classes() -> list[str]:\n",
    "    \"\"\"Return the canonical waste classification classes.\"\"\"\n",
    "    return [\n",
    "        'battery', 'biological', 'brown-glass', 'cardboard', 'clothes',\n",
    "        'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation functions\n",
    "def prepare_kaggle_datasets(datasets, raw_dir):\n",
    "    \"\"\"Copy datasets from /kaggle/input/ to working directory.\"\"\"\n",
    "    input_dir = \"/kaggle/input\"\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "    extracted = []\n",
    "    for dataset in datasets:\n",
    "        src = os.path.join(input_dir, dataset)\n",
    "        dst = os.path.join(raw_dir, dataset)\n",
    "        if os.path.exists(src):\n",
    "            print(f\"Copying {dataset}...\")\n",
    "            shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "            extracted.append(dst)\n",
    "        else:\n",
    "            print(f\"Warning: {src} not found\")\n",
    "\n",
    "    return extracted\n",
    "\n",
    "def consolidate_datasets(extracted_dirs: List[str], merged_dir: str) -> str:\n",
    "    \"\"\"Consolidate multiple datasets into a single directory structure.\"\"\"\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "    canonical_classes = get_canonical_classes()\n",
    "\n",
    "    # Class mapping for different datasets\n",
    "    class_mappings = {\n",
    "        # Add mappings if needed for different dataset formats\n",
    "    }\n",
    "\n",
    "    for dataset_dir in extracted_dirs:\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            continue\n",
    "\n",
    "        for root, dirs, files in os.walk(dataset_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    # Get class from directory name\n",
    "                    class_name = os.path.basename(root).lower()\n",
    "\n",
    "                    # Map to canonical class if needed\n",
    "                    canonical_class = class_mappings.get(class_name, class_name)\n",
    "\n",
    "                    if canonical_class in canonical_classes:\n",
    "                        # Create destination directory\n",
    "                        dest_dir = os.path.join(merged_dir, canonical_class)\n",
    "                        os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "                        # Copy file\n",
    "                        src_path = os.path.join(root, file)\n",
    "                        dest_path = os.path.join(dest_dir, f\"{os.path.basename(dataset_dir)}_{file}\")\n",
    "                        shutil.copy2(src_path, dest_path)\n",
    "\n",
    "    return merged_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset building functions\n",
    "def build_datasets(merged_dir: str, cfg: TrainConfig):\n",
    "    image_size = cfg.image_size\n",
    "    batch_size = cfg.batch_size\n",
    "    seed = cfg.seed\n",
    "\n",
    "    img_size = (image_size, image_size)\n",
    "    class_names = get_canonical_classes()\n",
    "\n",
    "    # Collect all files and labels\n",
    "    all_files = []\n",
    "    all_labels = []\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = Path(merged_dir) / class_name\n",
    "        if not class_dir.exists():\n",
    "            continue\n",
    "        for file in class_dir.glob(\"*\"):\n",
    "            if file.is_file() and file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                all_files.append(str(file))\n",
    "                all_labels.append(class_idx)\n",
    "\n",
    "    print(f\"Found {len(all_files)} images across {len(class_names)} classes\")\n",
    "\n",
    "    # Stratified split\n",
    "    label_counts = Counter(all_labels)\n",
    "    min_samples_per_class = 10\n",
    "\n",
    "    stratify_labels = [\n",
    "        label if label_counts[label] >= min_samples_per_class else -1 for label in all_labels\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
    "            all_files, all_labels, test_size=0.3, stratify=stratify_labels, random_state=seed\n",
    "        )\n",
    "        val_files, test_files, val_labels, test_labels = train_test_split(\n",
    "            temp_files,\n",
    "            temp_labels,\n",
    "            test_size=0.5,\n",
    "            stratify=[\n",
    "                label if label_counts[label] >= min_samples_per_class else -1\n",
    "                for label in temp_labels\n",
    "            ],\n",
    "            random_state=seed,\n",
    "        )\n",
    "        print(f\"Stratified split: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
    "    except ValueError:\n",
    "        train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
    "            all_files, all_labels, test_size=0.3, random_state=seed\n",
    "        )\n",
    "        val_files, test_files, val_labels, test_labels = train_test_split(\n",
    "            temp_files, temp_labels, test_size=0.5, random_state=seed\n",
    "        )\n",
    "        print(f\"Random split: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
    "\n",
    "    # Augmentation\n",
    "    brightness_factor = 0.1\n",
    "    contrast_factor = 0.1\n",
    "\n",
    "    aug = keras.Sequential([\n",
    "        keras.layers.Resizing(image_size, image_size),\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomRotation(0.1),\n",
    "        keras.layers.RandomZoom(0.1),\n",
    "        keras.layers.RandomContrast(contrast_factor),\n",
    "        keras.layers.RandomBrightness(brightness_factor),\n",
    "        keras.layers.RandomTranslation(0.05, 0.05),\n",
    "    ])\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def load_train_image(file_path, label):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = aug(img)\n",
    "        img = keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "        return img, tf.one_hot(label, len(class_names))\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def load_val_image(file_path, label):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, img_size)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img = keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "        return img, tf.one_hot(label, len(class_names))\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
    "    train_ds = train_ds.map(load_train_image, num_parallel_calls=AUTOTUNE)\n",
    "    train_ds = train_ds.shuffle(buffer_size=len(train_files), seed=seed)\n",
    "    train_ds = train_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_files, val_labels))\n",
    "    val_ds = val_ds.map(load_val_image, num_parallel_calls=AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_files, test_labels))\n",
    "    test_ds = test_ds.map(load_val_image, num_parallel_calls=AUTOTUNE)\n",
    "    test_ds = test_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, test_ds, class_names, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552811aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "def build_model(cfg: TrainConfig, class_names: list[str]):\n",
    "    if cfg.mixed_precision:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "    base = keras.applications.MobileNetV2(\n",
    "        input_shape=(cfg.image_size, cfg.image_size, 3), include_top=False, weights=\"imagenet\"\n",
    "    )\n",
    "\n",
    "    if cfg.fine_tune_from is not None:\n",
    "        base.trainable = True\n",
    "        for layer in base.layers[: cfg.fine_tune_from]:\n",
    "            layer.trainable = False\n",
    "        lr = 1e-3\n",
    "    else:\n",
    "        base.trainable = False\n",
    "        lr = 1e-3\n",
    "\n",
    "    inputs = keras.Input(shape=(cfg.image_size, cfg.image_size, 3))\n",
    "    x = base(inputs, training=True)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\", dtype=\"float32\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    try:\n",
    "        opt = keras.optimizers.legacy.Adam(learning_rate=lr, clipnorm=1.0)\n",
    "    except Exception:\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)\n",
    "\n",
    "    metrics = [\"accuracy\"]\n",
    "    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23301458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training function\n",
    "def main():\n",
    "    # Configuration\n",
    "    cfg = TrainConfig(\n",
    "        image_size=224,\n",
    "        batch_size=64,\n",
    "        epochs=15,\n",
    "        fine_tune_from=100,\n",
    "        model_dir='/kaggle/working/models',\n",
    "        datasets=[\n",
    "            \"garbage-classification\",\n",
    "            \"new-trash-classfication-dataset\",\n",
    "            \"realwaste\",\n",
    "            \"garbage-classification\",\n",
    "            \"garbage-classification\",\n",
    "            \"garbage-classification-v2\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"[bold cyan]Config:[/bold cyan]\")\n",
    "    for key, value in vars(cfg).items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Setup directories\n",
    "    base_model_dir = cfg.model_dir\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    versioned_model_dir = os.path.join(base_model_dir, f\"run_{timestamp}\")\n",
    "    cfg.model_dir = versioned_model_dir\n",
    "\n",
    "    print(f\"[bold green]Training Run:[/bold green] {timestamp}\")\n",
    "\n",
    "    # Prepare datasets\n",
    "    raw_dir = \"/kaggle/working/raw_datasets\"\n",
    "    merged_dir = \"/kaggle/working/merged_dataset\"\n",
    "\n",
    "    print(f\"[bold cyan]Preparing datasets from /kaggle/input/[/bold cyan]\")\n",
    "    extracted = prepare_kaggle_datasets(cfg.datasets, raw_dir)\n",
    "    merged = consolidate_datasets(extracted, merged_dir)\n",
    "\n",
    "    # Build datasets\n",
    "    train_ds, val_ds, test_ds, class_names, train_labels, val_labels, test_labels = build_datasets(merged, cfg)\n",
    "\n",
    "    print(f\"Dataset statistics:\")\n",
    "    print(f\"  Classes: {class_names}\")\n",
    "    print(f\"  Training samples: {len(train_labels)}\")\n",
    "    print(f\"  Validation samples: {len(val_labels)}\")\n",
    "    print(f\"  Test samples: {len(test_labels)}\")\n",
    "\n",
    "    # Class weights\n",
    "    if cfg.no_class_weights:\n",
    "        class_weight = None\n",
    "    else:\n",
    "        label_counts = Counter(train_labels)\n",
    "        total = sum(label_counts.values())\n",
    "        num_classes = len(class_names)\n",
    "        class_weight = {}\n",
    "        for i in range(num_classes):\n",
    "            count = label_counts.get(i, 0)\n",
    "            if count > 0:\n",
    "                class_weight[i] = total / (num_classes * count)\n",
    "            else:\n",
    "                class_weight[i] = 10.0\n",
    "        print(f\"Class weights: {class_weight}\")\n",
    "\n",
    "    # Build model\n",
    "    model = build_model(cfg, class_names)\n",
    "    print(model.summary())\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(cfg.model_dir, \"waste_classifier_best.keras\"), save_best_only=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_accuracy\", factor=0.5, patience=5, min_lr=1e-7, verbose=1\n",
    "        ),\n",
    "    ]\n",
    "    os.makedirs(cfg.model_dir, exist_ok=True)\n",
    "\n",
    "    # Save class weights\n",
    "    weights_path = os.path.join(cfg.model_dir, \"class_weights.json\")\n",
    "    with open(weights_path, \"w\") as f:\n",
    "        if class_weight is not None:\n",
    "            json.dump({class_names[i]: w for i, w in class_weight.items()}, f, indent=2)\n",
    "        else:\n",
    "            json.dump({\"disabled\": True}, f, indent=2)\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=cfg.epochs,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    h5_path = os.path.join(cfg.model_dir, \"waste_classifier.h5\")\n",
    "    model.save(h5_path)\n",
    "    model.save(os.path.join(cfg.model_dir, \"waste_classifier.keras\"))\n",
    "    print(f\"Saved model to {h5_path}\")\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=1)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # Save labels\n",
    "    labels_path = os.path.join(cfg.model_dir, \"labels.json\")\n",
    "    with open(labels_path, \"w\") as f:\n",
    "        json.dump(class_names, f)\n",
    "\n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"config\": vars(cfg),\n",
    "        \"final_accuracy\": float(test_acc),\n",
    "        \"final_loss\": float(test_loss),\n",
    "        \"class_names\": class_names,\n",
    "        \"training_samples\": len(train_labels),\n",
    "        \"validation_samples\": len(val_labels),\n",
    "        \"test_samples\": len(test_labels),\n",
    "    }\n",
    "\n",
    "    metadata_path = os.path.join(versioned_model_dir, \"training_metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"Training complete! Model saved in: {versioned_model_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e378e2e",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "After training completes:\n",
    "1. Check the Output tab for model files\n",
    "2. Download the `models/` folder\n",
    "3. The trained model will be in `run_YYYYMMDD_HHMMSS/`\n",
    "\n",
    "Expected performance:\n",
    "- First epoch accuracy: >0.3\n",
    "- Final accuracy: 0.7-0.8\n",
    "- Training time: ~20-40 minutes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
