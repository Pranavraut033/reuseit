{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1962185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix protobuf and TensorFlow compatibility issues\n",
    "!pip uninstall protobuf -y --quiet\n",
    "!pip install protobuf==3.20.3 --quiet\n",
    "!pip install tensorflow --upgrade --quiet\n",
    "\n",
    "print(\"Protobuf and TensorFlow updated. Please restart runtime if you encounter import errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a612b2",
   "metadata": {},
   "source": [
    "## Troubleshooting TensorFlow/Protobuf Errors\n",
    "\n",
    "If you see `MessageFactory` errors or CUDA warnings:\n",
    "\n",
    "1. **Restart the runtime**: Runtime â†’ Restart Runtime\n",
    "2. **Run the protobuf fix cell** (Cell 2) first\n",
    "3. **Then run the install cell** (Cell 4)\n",
    "4. **Restart again** if needed\n",
    "5. **Continue with the rest of the cells**\n",
    "\n",
    "The errors are due to version conflicts between TensorFlow and protobuf. The fixes above should resolve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f1444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle API\n",
    "from google.colab import files\n",
    "print(\"Please upload your kaggle.json file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Setup Kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"Kaggle API setup complete!\")\n",
    "\n",
    "\n",
    "# # Setup Kaggle API (hardcoded credentials)\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# # Create kaggle.json with credentials\n",
    "# kaggle_credentials = {\n",
    "#     \"username\": \"xx\",\n",
    "#     \"key\": \"xxx\"\n",
    "# }\n",
    "\n",
    "# # Setup Kaggle directory\n",
    "# os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "\n",
    "# # Write credentials file\n",
    "# kaggle_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n",
    "# with open(kaggle_path, \"w\") as f:\n",
    "#     json.dump(kaggle_credentials, f)\n",
    "\n",
    "# # Set permissions\n",
    "# os.chmod(kaggle_path, 0o600)\n",
    "\n",
    "# print(\"Kaggle API setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run after protobuf fix)\n",
    "!pip install rich scikit-learn tensorflow-addons --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05111cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and configure TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU DETECTION & CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List all GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\nPhysical GPUs: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    print(f\"  - {gpu}\")\n",
    "\n",
    "# Check CUDA/cuDNN\n",
    "if gpus:\n",
    "    print(\"\\nâœ… GPU is AVAILABLE!\")\n",
    "    print(f\"TensorFlow built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "    print(f\"GPU device available: {tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)}\")\n",
    "\n",
    "    # Enable memory growth to prevent TensorFlow from allocating all GPU memory at once\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"Memory growth enabled for {gpu.name}\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error setting memory growth: {e}\")\n",
    "\n",
    "    # Get GPU details\n",
    "    print(\"\\nðŸ“Š GPU Details:\")\n",
    "    gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
    "    for key, value in gpu_details.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"\\nâŒ NO GPU DETECTED - Training will use CPU only!\")\n",
    "    print(\"In Colab: Go to Runtime â†’ Change runtime type â†’ Set Hardware accelerator to 'GPU' or 'T4 GPU'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9786c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries (with error handling)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "\n",
    "try:\n",
    "    import argparse\n",
    "    import json\n",
    "    import shutil\n",
    "    from datetime import datetime\n",
    "    from typing import List, Dict\n",
    "    from pathlib import Path\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "\n",
    "    # Suppress more warnings\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "    from rich import print\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from collections import Counter\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    print(\"All imports successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "    # Fallback imports\n",
    "    import argparse\n",
    "    import json\n",
    "    import shutil\n",
    "    from datetime import datetime\n",
    "    from typing import List, Dict\n",
    "    from pathlib import Path\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from collections import Counter\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    print(\"Basic imports successful (without rich)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e154c4",
   "metadata": {},
   "source": [
    "## âš¡ Performance Tips\n",
    "\n",
    "**If training is slow:**\n",
    "\n",
    "1. **Verify GPU is active**: Run the GPU check cell above - you should see \"GPU is AVAILABLE\"\n",
    "2. **In Google Colab**: Runtime â†’ Change runtime type â†’ Hardware accelerator = **T4 GPU** (or A100/V100 if available)\n",
    "3. **Batch size**: Increase from 64 to 128 or 256 if you have GPU memory available\n",
    "4. **Mixed precision**: Enable by setting `mixed_precision=True` in config (2-3x speedup on modern GPUs)\n",
    "5. **Check GPU utilization**: Run `!nvidia-smi` in a cell to see if GPU is being used during training\n",
    "\n",
    "Expected speeds:\n",
    "- **CPU only**: ~200-300 seconds/epoch (very slow)\n",
    "- **T4 GPU**: ~30-50 seconds/epoch\n",
    "- **V100/A100**: ~10-20 seconds/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "class TrainConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.image_size = kwargs.get('image_size', 224)\n",
    "        self.batch_size = kwargs.get('batch_size', 64)\n",
    "        self.epochs = kwargs.get('epochs', 15)\n",
    "        self.validation_split = kwargs.get('validation_split', 0.2)\n",
    "        self.fine_tune_from = kwargs.get('fine_tune_from', 100)\n",
    "        self.mixed_precision = kwargs.get('mixed_precision', False)\n",
    "        self.model_dir = kwargs.get('model_dir', '/content/models')\n",
    "        self.datasets = kwargs.get('datasets', [])\n",
    "        self.no_class_weights = kwargs.get('no_class_weights', False)\n",
    "        self.repr_samples = kwargs.get('repr_samples', 100)\n",
    "        self.brightness_factor = kwargs.get('brightness_factor', 0.1)\n",
    "        self.contrast_factor = kwargs.get('contrast_factor', 0.1)\n",
    "        self.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical classes mapping\n",
    "def get_canonical_classes() -> list[str]:\n",
    "    \"\"\"Return the canonical waste classification classes.\"\"\"\n",
    "    return [\"paper_cardboard\", \"glass\", \"recyclables\", \"bio_waste\", \"textile_reuse\", \"electronics\", \"battery\", \"residual_waste\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c65934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_datasets(base_dir, merged_dir):\n",
    "    \"\"\"\n",
    "    Consolidate multiple Kaggle datasets into a unified structure.\n",
    "    Maps various class names to standardized categories.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Define dataset configurations with their paths and class mappings\n",
    "    datasets = [\n",
    "        {\n",
    "            'name': 'asdasdasasdas/garbage-classification',\n",
    "            'slug': 'asdasdasasdas_garbage-classification',\n",
    "            'path': 'Garbage classification/Garbage classification',\n",
    "            'class_mappings': {\n",
    "                'cardboard': 'paper_cardboard',\n",
    "                'glass': 'glass',\n",
    "                'metal': 'recyclables',\n",
    "                'paper': 'paper_cardboard',\n",
    "                'plastic': 'recyclables',\n",
    "                'trash': 'residual_waste'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'glhdamar/new-trash-classfication-dataset',\n",
    "            'slug': 'glhdamar_new-trash-classfication-dataset',\n",
    "            'path': 'new-dataset-trash-type-v2',\n",
    "            'class_mappings': {\n",
    "                'cardboard': 'paper_cardboard',\n",
    "                'glass': 'glass',\n",
    "                'metal': 'recyclables',\n",
    "                'paper': 'paper_cardboard',\n",
    "                'plastic': 'recyclables',\n",
    "                'trash': 'residual_waste',\n",
    "                'e-waste': 'electronics',  # Map e-waste to electronics\n",
    "                'organic': 'bio_waste',  # Map organic to bio_waste\n",
    "                'textile': 'textile_reuse'   # Map textile to textile_reuse\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'joebeachcapital/realwaste',\n",
    "            'slug': 'joebeachcapital_realwaste',\n",
    "            'path': 'realwaste-main/RealWaste',\n",
    "            'class_mappings': {\n",
    "                'Cardboard': 'paper_cardboard',\n",
    "                'Glass': 'glass',\n",
    "                'Metal': 'recyclables',\n",
    "                'Paper': 'paper_cardboard',\n",
    "                'Plastic': 'recyclables',\n",
    "                'Miscellaneous Trash': 'residual_waste',\n",
    "                'Food Organics': 'bio_waste',    # Map to bio_waste\n",
    "                'Textile Trash': 'textile_reuse',    # Map to textile_reuse\n",
    "                'Vegetation': 'bio_waste'        # Map to bio_waste\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'mostafaabla/garbage-classification',\n",
    "            'slug': 'mostafaabla_garbage-classification',\n",
    "            'path': 'garbage_classification',\n",
    "            'class_mappings': {\n",
    "                'cardboard': 'paper_cardboard',\n",
    "                'metal': 'recyclables',\n",
    "                'paper': 'paper_cardboard',\n",
    "                'plastic': 'recyclables',\n",
    "                'trash': 'residual_waste',\n",
    "                'battery': 'battery',      # Map battery to battery\n",
    "                'biological': 'bio_waste',   # Map biological to bio_waste\n",
    "                'clothes': 'textile_reuse',      # Map clothes to textile_reuse\n",
    "                'shoes': 'textile_reuse',        # Map shoes to textile_reuse\n",
    "                'brown-glass': 'glass',  # Map glass variants to glass\n",
    "                'green-glass': 'glass',\n",
    "                'white-glass': 'glass'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'karansolanki01/garbage-classification',\n",
    "            'slug': 'karansolanki01_garbage-classification',\n",
    "            'path': 'Data/Garbage_Classification',\n",
    "            'class_mappings': {\n",
    "                'Cardboard': 'paper_cardboard',\n",
    "                'Glass': 'glass',\n",
    "                'Metal': 'recyclables',\n",
    "                'Paper': 'paper_cardboard',\n",
    "                'Plastic': 'recyclables',\n",
    "                'Battery': 'battery',   # Map Battery to battery\n",
    "                'Clothes': 'textile_reuse'    # Map Clothes to textile_reuse\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'sumn2u/garbage-classification-v2',\n",
    "            'slug': 'sumn2u_garbage-classification-v2',\n",
    "            'path': 'garbage-dataset',\n",
    "            'class_mappings': {\n",
    "                'cardboard': 'paper_cardboard',\n",
    "                'glass': 'glass',\n",
    "                'metal': 'recyclables',\n",
    "                'paper': 'paper_cardboard',\n",
    "                'plastic': 'recyclables',\n",
    "                'trash': 'residual_waste',\n",
    "                'battery': 'battery',     # Map battery to battery\n",
    "                'biological': 'bio_waste',  # Map biological to bio_waste\n",
    "                'clothes': 'textile_reuse',     # Map clothes to textile_reuse\n",
    "                'shoes': 'textile_reuse'        # Map shoes to textile_reuse\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Create consolidated directory structure\n",
    "    consolidated_dir = Path(merged_dir)\n",
    "    target_classes = [\"paper_cardboard\", \"glass\", \"recyclables\", \"bio_waste\", \"textile_reuse\", \"electronics\", \"battery\", \"residual_waste\"]\n",
    "\n",
    "    for cls in target_classes:\n",
    "        (consolidated_dir / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Consolidating datasets into {consolidated_dir}\")\n",
    "    total_images = 0\n",
    "\n",
    "    for dataset in datasets:\n",
    "        # Use slug for directory path (the actual extracted directory name)\n",
    "        dataset_path = Path(base_dir) / dataset['slug'] / dataset['path']\n",
    "        print(f\"\\nProcessing dataset: {dataset['name']}\")\n",
    "        print(f\"Looking in: {dataset_path}\")\n",
    "\n",
    "        if not dataset_path.exists():\n",
    "            print(f\"Warning: Dataset path {dataset_path} does not exist, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Process each source class\n",
    "        for source_class, target_class in dataset['class_mappings'].items():\n",
    "            source_dir = dataset_path / source_class\n",
    "            target_dir = consolidated_dir / target_class\n",
    "\n",
    "            if source_dir.exists():\n",
    "                images = list(source_dir.glob('*'))\n",
    "                print(f\"  {source_class} -> {target_class}: {len(images)} images\")\n",
    "\n",
    "                for img_path in images:\n",
    "                    if img_path.is_file() and img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                        shutil.copy2(img_path, target_dir / f\"{dataset['slug']}_{img_path.name}\")\n",
    "                        total_images += 1\n",
    "            else:\n",
    "                print(f\"  {source_class} -> {target_class}: directory not found\")\n",
    "\n",
    "    print(f\"\\nConsolidation complete! Total images copied: {total_images}\")\n",
    "\n",
    "    # Print final class counts\n",
    "    print(\"\\nFinal class distribution:\")\n",
    "    for cls in target_classes:\n",
    "        count = len(list((consolidated_dir / cls).glob('*')))\n",
    "        print(f\"  {cls}: {count} images\")\n",
    "\n",
    "    return str(consolidated_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37bbed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset building functions\n",
    "def build_datasets(merged_dir: str, cfg: TrainConfig):\n",
    "    image_size = cfg.image_size\n",
    "    batch_size = cfg.batch_size\n",
    "    seed = cfg.seed\n",
    "\n",
    "    img_size = (image_size, image_size)\n",
    "    class_names = get_canonical_classes()\n",
    "\n",
    "    # Collect all files and labels\n",
    "    all_files = []\n",
    "    all_labels = []\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = Path(merged_dir) / class_name\n",
    "        if not class_dir.exists():\n",
    "            continue\n",
    "        for file in class_dir.glob(\"*\"):\n",
    "            if file.is_file() and file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                all_files.append(str(file))\n",
    "                all_labels.append(class_idx)\n",
    "\n",
    "    print(f\"Found {len(all_files)} images across {len(class_names)} classes\")\n",
    "\n",
    "    # Stratified split\n",
    "    label_counts = Counter(all_labels)\n",
    "    min_samples_per_class = 10\n",
    "\n",
    "    stratify_labels = [\n",
    "        label if label_counts[label] >= min_samples_per_class else -1 for label in all_labels\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
    "            all_files, all_labels, test_size=0.3, stratify=stratify_labels, random_state=seed\n",
    "        )\n",
    "        val_files, test_files, val_labels, test_labels = train_test_split(\n",
    "            temp_files,\n",
    "            temp_labels,\n",
    "            test_size=0.5,\n",
    "            stratify=[\n",
    "                label if label_counts[label] >= min_samples_per_class else -1\n",
    "                for label in temp_labels\n",
    "            ],\n",
    "            random_state=seed,\n",
    "        )\n",
    "        print(f\"Stratified split: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
    "    except ValueError:\n",
    "        train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
    "            all_files, all_labels, test_size=0.3, random_state=seed\n",
    "        )\n",
    "        val_files, test_files, val_labels, test_labels = train_test_split(\n",
    "            temp_files, temp_labels, test_size=0.5, random_state=seed\n",
    "        )\n",
    "        print(f\"Random split: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test\")\n",
    "\n",
    "    # Augmentation\n",
    "    brightness_factor = 0.1\n",
    "    contrast_factor = 0.1\n",
    "\n",
    "    aug = keras.Sequential([\n",
    "        keras.layers.Resizing(image_size, image_size),\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomRotation(0.1),\n",
    "        keras.layers.RandomZoom(0.1),\n",
    "        keras.layers.RandomContrast(contrast_factor),\n",
    "        keras.layers.RandomBrightness(brightness_factor),\n",
    "        keras.layers.RandomTranslation(0.05, 0.05),\n",
    "    ])\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def load_train_image(file_path, label):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = aug(img)\n",
    "        img = keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "        return img, tf.one_hot(label, len(class_names))\n",
    "\n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def load_val_image(file_path, label):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, img_size)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img = keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "        return img, tf.one_hot(label, len(class_names))\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_files, train_labels))\n",
    "    train_ds = train_ds.map(load_train_image, num_parallel_calls=AUTOTUNE)\n",
    "    # Reduced shuffle buffer size to prevent memory issues\n",
    "    train_ds = train_ds.shuffle(buffer_size=min(1000, len(train_files)), seed=seed)\n",
    "    train_ds = train_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_files, val_labels))\n",
    "    val_ds = val_ds.map(load_val_image, num_parallel_calls=AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_files, test_labels))\n",
    "    test_ds = test_ds.map(load_val_image, num_parallel_calls=AUTOTUNE)\n",
    "    test_ds = test_ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, test_ds, class_names, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "def build_model(cfg: TrainConfig, class_names: list[str]):\n",
    "    if cfg.mixed_precision:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "    base = keras.applications.MobileNetV2(\n",
    "        input_shape=(cfg.image_size, cfg.image_size, 3), include_top=False, weights=\"imagenet\"\n",
    "    )\n",
    "\n",
    "    if cfg.fine_tune_from is not None:\n",
    "        base.trainable = True\n",
    "        for layer in base.layers[: cfg.fine_tune_from]:\n",
    "            layer.trainable = False\n",
    "        lr = 1e-3\n",
    "    else:\n",
    "        base.trainable = False\n",
    "        lr = 1e-3\n",
    "\n",
    "    inputs = keras.Input(shape=(cfg.image_size, cfg.image_size, 3))\n",
    "    x = base(inputs, training=True)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    outputs = keras.layers.Dense(len(class_names), activation=\"softmax\", dtype=\"float32\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    try:\n",
    "        opt = keras.optimizers.legacy.Adam(learning_rate=lr, clipnorm=1.0)\n",
    "    except Exception:\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)\n",
    "\n",
    "    metrics = [\"accuracy\"]\n",
    "    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle download function\n",
    "def ensure_kaggle_download(datasets: list[str], raw_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Download datasets from Kaggle if not already present.\n",
    "    Returns the raw_dir path.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import zipfile\n",
    "    import kaggle\n",
    "    from pathlib import Path\n",
    "\n",
    "    os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "    for dataset in datasets:\n",
    "        slug = dataset.replace(\"/\", \"_\")\n",
    "        zip_name = dataset.split(\"/\")[-1] + \".zip\"\n",
    "        target_zip = Path(raw_dir) / zip_name\n",
    "        target_dir = Path(raw_dir) / slug\n",
    "\n",
    "        # If already extracted, skip everything\n",
    "        if target_dir.exists():\n",
    "            print(f\"Dataset {dataset} already exists at {target_dir}, skipping download.\")\n",
    "            continue\n",
    "\n",
    "        # If zip exists, skip download\n",
    "        if not target_zip.exists():\n",
    "            print(f\"Downloading dataset: {dataset}\")\n",
    "            try:\n",
    "                kaggle.api.dataset_download_files(dataset, path=raw_dir, quiet=False, unzip=False)\n",
    "                print(f\"Downloaded {dataset}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {dataset}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Zip already exists for {dataset}, skipping download.\")\n",
    "\n",
    "        # Extract to dataset-specific directory\n",
    "        print(f\"Extracting {target_zip} to {target_dir}...\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(target_zip, \"r\") as zf:\n",
    "                zf.extractall(target_dir)\n",
    "            print(f\"Extracted {dataset}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting {dataset}: {e}\")\n",
    "\n",
    "    return raw_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f415454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training function\n",
    "def main():\n",
    "    # Enable mixed precision for faster training on GPU (2-3x speedup)\n",
    "    # This uses float16 for computations but float32 for variables to maintain accuracy\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"ðŸš€ Enabling mixed precision training for GPU acceleration...\")\n",
    "        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "        print(\"âœ… Mixed precision enabled: computations in float16, variables in float32\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Running on CPU - mixed precision disabled\")\n",
    "\n",
    "    # Configuration\n",
    "    cfg = TrainConfig(\n",
    "        image_size=224,\n",
    "        batch_size=128,  # Increased from 64 - adjust based on GPU memory\n",
    "        epochs=15,\n",
    "        fine_tune_from=100,\n",
    "        model_dir='/content/models',\n",
    "        datasets=[\n",
    "            \"asdasdasasdas/garbage-classification\",\n",
    "            \"glhdamar/new-trash-classfication-dataset\",\n",
    "            \"joebeachcapital/realwaste\",\n",
    "            \"mostafaabla/garbage-classification\",\n",
    "            \"karansolanki01/garbage-classification\",\n",
    "            \"sumn2u/garbage-classification-v2\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"[bold cyan]Config:[/bold cyan]\")\n",
    "    for key, value in vars(cfg).items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Setup directories\n",
    "    base_model_dir = cfg.model_dir\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    versioned_model_dir = os.path.join(base_model_dir, f\"run_{timestamp}\")\n",
    "    cfg.model_dir = versioned_model_dir\n",
    "\n",
    "    print(f\"[bold green]Training Run:[/bold green] {timestamp}\")\n",
    "\n",
    "    # Prepare datasets (Colab downloads from Kaggle)\n",
    "    raw_dir = \"/content/raw_datasets\"\n",
    "    merged_dir = \"/content/merged_dataset\"\n",
    "\n",
    "    print(f\"[bold cyan]Downloading datasets from Kaggle[/bold cyan]\")\n",
    "    extracted = ensure_kaggle_download(cfg.datasets, raw_dir)\n",
    "    merged = consolidate_datasets(extracted, merged_dir)\n",
    "\n",
    "    # Build datasets\n",
    "    train_ds, val_ds, test_ds, class_names, train_labels, val_labels, test_labels = build_datasets(merged, cfg)\n",
    "\n",
    "    print(f\"Dataset statistics:\")\n",
    "    print(f\"  Classes: {class_names}\")\n",
    "    print(f\"  Training samples: {len(train_labels)}\")\n",
    "    print(f\"  Validation samples: {len(val_labels)}\")\n",
    "    print(f\"  Test samples: {len(test_labels)}\")\n",
    "\n",
    "    # Class weights\n",
    "    if cfg.no_class_weights:\n",
    "        class_weight = None\n",
    "    else:\n",
    "        label_counts = Counter(train_labels)\n",
    "        total = sum(label_counts.values())\n",
    "        num_classes = len(class_names)\n",
    "        class_weight = {}\n",
    "        for i in range(num_classes):\n",
    "            count = label_counts.get(i, 0)\n",
    "            if count > 0:\n",
    "                class_weight[i] = total / (num_classes * count)\n",
    "            else:\n",
    "                class_weight[i] = 10.0\n",
    "        print(f\"Class weights: {class_weight}\")\n",
    "\n",
    "    # Build model\n",
    "    model = build_model(cfg, class_names)\n",
    "    print(model.summary())\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(cfg.model_dir, \"waste_classifier_best.keras\"), save_best_only=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_accuracy\", factor=0.5, patience=5, min_lr=1e-7, verbose=1\n",
    "        ),\n",
    "    ]\n",
    "    os.makedirs(cfg.model_dir, exist_ok=True)\n",
    "\n",
    "    # Save class weights\n",
    "    weights_path = os.path.join(cfg.model_dir, \"class_weights.json\")\n",
    "    with open(weights_path, \"w\") as f:\n",
    "        if class_weight is not None:\n",
    "            json.dump({class_names[i]: w for i, w in class_weight.items()}, f, indent=2)\n",
    "        else:\n",
    "            json.dump({\"disabled\": True}, f, indent=2)\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=cfg.epochs,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight,\n",
    "        verbose=1,  # Changed from 2 to 1 for progress bars\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    h5_path = os.path.join(cfg.model_dir, \"waste_classifier.h5\")\n",
    "    model.save(h5_path)\n",
    "    model.save(os.path.join(cfg.model_dir, \"waste_classifier.keras\"))\n",
    "    print(f\"Saved model to {h5_path}\")\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"Evaluating on test set...\")\n",
    "    test_loss, test_acc = model.evaluate(test_ds, verbose=1)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # Save labels\n",
    "    labels_path = os.path.join(cfg.model_dir, \"labels.json\")\n",
    "    with open(labels_path, \"w\") as f:\n",
    "        json.dump(class_names, f)\n",
    "\n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"config\": vars(cfg),\n",
    "        \"final_accuracy\": float(test_acc),\n",
    "        \"final_loss\": float(test_loss),\n",
    "        \"class_names\": class_names,\n",
    "        \"training_samples\": len(train_labels),\n",
    "        \"validation_samples\": len(val_labels),\n",
    "        \"test_samples\": len(test_labels),\n",
    "    }\n",
    "\n",
    "    metadata_path = os.path.join(versioned_model_dir, \"training_metadata.json\")\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"Training complete! Model saved in: {versioned_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028fb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU usage during training (run this in a separate cell while training)\n",
    "!watch -n 1 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b976ff",
   "metadata": {},
   "source": [
    "## ðŸ” Troubleshooting Slow Training\n",
    "\n",
    "**If your training is as slow as CPU:**\n",
    "\n",
    "### 1. Check GPU is Active\n",
    "Run this command to verify GPU is being used:\n",
    "```python\n",
    "!nvidia-smi\n",
    "```\n",
    "You should see:\n",
    "- GPU name (e.g., \"Tesla T4\" or \"Tesla V100\")\n",
    "- GPU utilization % (should be 80-100% during training)\n",
    "- Memory usage (should be several GB)\n",
    "\n",
    "### 2. Verify Mixed Precision is Enabled\n",
    "After running the main() function, you should see:\n",
    "```\n",
    "ðŸš€ Enabling mixed precision training for GPU acceleration...\n",
    "âœ… Mixed precision enabled\n",
    "```\n",
    "\n",
    "### 3. Common Issues:\n",
    "\n",
    "**âŒ GPU shows 0% utilization during training:**\n",
    "- The model might be running on CPU despite GPU being available\n",
    "- Try restarting the runtime and running cells in order\n",
    "- Check if TensorFlow sees the GPU: `tf.config.list_physical_devices('GPU')`\n",
    "\n",
    "**âŒ \"ResourceExhausted\" or \"Out of Memory\" errors:**\n",
    "- Reduce batch size: Change `batch_size=128` to `batch_size=64` or `batch_size=32`\n",
    "- The notebook will automatically retry with smaller batch\n",
    "\n",
    "**âŒ Training speed same as laptop (200-300 sec/epoch):**\n",
    "- You're likely running on CPU\n",
    "- **Colab users**: Go to **Runtime** â†’ **Change runtime type** â†’ Set **Hardware accelerator** to **GPU**\n",
    "- Disconnect and reconnect to get a GPU instance\n",
    "\n",
    "### 4. Expected Performance:\n",
    "- **CPU only**: 200-300 seconds/epoch âš ï¸ (Very slow)\n",
    "- **T4 GPU**: 30-50 seconds/epoch âœ…\n",
    "- **V100/A100 GPU**: 10-20 seconds/epoch ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d097eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show GPU status before training\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"âœ… Training will use GPU: {gpus[0].name}\")\n",
    "    print(\"ðŸ’¡ Tip: Run !nvidia-smi in another cell to monitor GPU usage in real-time\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: No GPU detected! Training will be SLOW on CPU.\")\n",
    "    print(\"In Colab: Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Start training\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdaf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained model (run after training completes)\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Zip the models folder\n",
    "shutil.make_archive('/content/models', 'zip', '/content/models')\n",
    "\n",
    "# Download\n",
    "files.download('/content/models.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5d966",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "After training completes:\n",
    "1. Run the download cell above to get your trained model\n",
    "2. The model files will be in `run_YYYYMMDD_HHMMSS/` folder\n",
    "\n",
    "Expected performance:\n",
    "- First epoch accuracy: >0.3\n",
    "- Final accuracy: 0.7-0.8\n",
    "- Training time: ~30-60 minutes (Colab T4 GPU)\n",
    "\n",
    "## Notes\n",
    "- Colab sessions disconnect after 12 hours\n",
    "- Save your work regularly\n",
    "- Models are saved to `/content/models/`\n",
    "- Use the download cell to get your trained model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
